# 决策树



## 特征选择

* 防止模型过深
* 防止模型过宽



**信息增益（information gain）**

> 原数据集的熵 减去 划分后的熵， 增益越大越好，即：划分后的熵越小越好


$$
\text{Gain}(D, a) = \text{Ent}(D) - \sum_{v=1}^V\frac{|D^v|}{|D|} \text{Ent}(D^v)
$$
**信息增益比**

> 信息增益模型对取值比较多的模型会有偏好，这种偏好会带来不利影响，
>
> 1. 模型过于复杂
> 2. 过拟合
>
> 举例：假设有个任务，根据今天的天气情况来推测明天的天气。其中有个日期特征，那么这个日期特征一定是个信息增益最大的目标，但是日期却不能作为到底是啥天气的评价标准。
>
> 信息增益比对 取值数目较少的属性有所偏好。


$$
\text{Gain_ratio(D,a)} = \frac{\text{Gain(D, a)}}{\text{IV(D, a)}}
$$
$\text{IV(D,a)}$ 即 特征 $a$ 作为随机变量的熵。



**基尼指数**

> 数据集 D 的纯度可用基尼值来度量

$$
\text{Gini(p)} = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2
$$

* 有 $K$ 个类，第 $k$ 个类的概率是 $p_k$ 。

$$
\text{Gini_index(D,a)} = \sum_{v=1}^V \frac{|D^v|}{|D|} \text{Gini}(D^v)
$$

* `Gini_index` 越低越好。



## 剪枝策略

* 预剪枝
  * 决策树生成过程中，对每个节点的划分进行估计，如果无法提供更好的泛化性能，放弃。
* 后剪枝
  * 先生成一个完整的决策树，然后再自底向上的进行剪枝。
  * 看剪枝后，能否提升验证集精度



## 特征处理

**连续值处理**

* 量化 + 二分法
  * 将特征的所有值从小到大排序，去中间的值

**缺失值处理**

