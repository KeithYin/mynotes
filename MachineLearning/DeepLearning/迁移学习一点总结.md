# 迁移学习一点总结



##  How transferable are features in deep neural networks

* How transferable are features in deep neural networks
  * 可转移性受到两个截然不同的问题的影响
    * 将高层次的神经元特殊化为原始任务，以牺牲在目标任务上的性能为代价，这是意料之中的。
    * optimization difficulties related to splitting networks between co-adapted neurons, which was not expected
  * 结论
    * 随着原始任务与目标任务的距离越来越远，可转移性越来越低，但迁移也比随机初始化要好
    * 对任意数量的层进行迁移都会增强网络泛化能力



不同的数据集，不同的目标函数，学习到的第一层核 都像一个`Gabor Filter or color blobs`，称之为`general feature` , 最后一层，和目标任务相关，称之为`specific feature`。

有了`general` 和 `specific` 的概念后，需要考虑下面三个问题：

* 如何定量的描述每层的 `general` 或者 `specific` 的程度
* 从`general`到`specific`的变化是突然间的变化，还是逐渐的变化
* 从`general`到`specific`的转移发生在什么地方：离第一层近，中间层，还是最后一层？



迁移学习的一般套路是：先在一个基准数据集上（ImageNet）训练一个基准网络，然后把训练好的网络的前几层拿出来给目标任务用。

* 是否要 fine-tune
  * 如果不fine-tune的话，只需把迁移过来的参数冻住就可以了。只训练随机初始化参数的那几层
  * 是否fine-tune取决于
    * 目标数据集的大小
    * 迁移过来的参数的数量
    * 如果目标数据集太小然后迁移过来的参数很多，fine-tune可能会导致过拟合
    * 当然如果目标数据集比较大，那么是可以fine-tune的
    * 入股数据集过大的话，就没必要迁移学习了



实验设定：

`ImageNet` 1000类数据，分成两份(`A, B`)，一份500类。

* 框架
  * baseA:  8层的卷积神经网络，用数据集`A`训练
  * baseB: 8层的卷积神经网络，用数据集`B`训练
  * B3B:  8层的卷积神经网络，取`baseB`的前三层，然后再用数据集`B`训练
  * A3B:  8层的卷积神经网络，取`baseA`的前三层，然后再用数据集`B`训练
* 可能结果分析
  * B3B和A3B的效果一样好的话：说明，前三层的特征还是通用的
  * 如果A3B的效果没有B3B好：说明第三层的特征有点 `specific`了
* 对比试验结果及其分析：
  * baseA和baseB的验证集错误率是小于1000类时的错误率的，原因：只有500类，有更少的机会犯错
  * B1B的结果和baseB十分接近，B2B，B3B的结果也和baseB接近，但是B4-5B的结果就不行了。出现这种现象的原因是：原始网络 `连续层之间` 包含脆弱的 `co-adapted features`。这些特征不能单独的由上层学习出来，需要一起学习才有可能。 B6B，B7B的结果重新又开始接近baseB了，说明：6-7， 7-8之间有更少的 `co-adaptation features`。 得出的结论是：迁移学习的时候，从中间开始迁移，有可能更加难以训练。
  * 加上finetune的网络不会出现上述 B4B，B5B出现的问题，`joint training` 在有些时候是必须的
  * A1B,A2B的结果和baseB基本一致，说明第一层核第二层的特征比较`general`，A3B-A7B的效果逐渐降低
  * A1B的训练效果优于baseB。
  * 迁移过去的特征加上finetune会使得网络泛化的优于那些从头训练的网络结构