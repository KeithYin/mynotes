{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**机器学习分类** (按照数据集和要做的任务进行分类)\n",
    "* 监督学习\n",
    "* 无监督学习\n",
    "* 半监督学习\n",
    "* 强化学习\n",
    "\n",
    "**数据可视化** \n",
    "* 标签分布可视化 (使用直方图看看分布就好了)\n",
    "* 特征分布可视化 (可以根据特征所属的不同的标签 标色, 一般使用 scatter)\n",
    "\n",
    "## 监督学习\n",
    "$\\Bbb P(y| \\mathbf x_i, D)$, 后验概率\n",
    "\n",
    "## 无监督学习\n",
    "> knowledge discovery\n",
    "\n",
    "* 密度估计(density estimation)\n",
    "* vae\n",
    "* gan\n",
    "* normalizing flow (这个是不是属于 density estimation 中的? 确定一下)\n",
    "\n",
    "\n",
    "**概率估计**\n",
    "\n",
    "建模为 : $\\Bbb P(\\mathbf x_i|\\mathbf \\theta)$, 给定一个样本,能够得到其 概率\n",
    "* 注意和 监督学习的区别 :  $\\Bbb P(\\mathbf x_i|\\mathbf \\theta)$ VS $\\Bbb P(y| \\mathbf x_i, D)$\n",
    "* 监督学习: 目标 条件概率密度估计\n",
    "* 无监督学习: 目标为 无条件 概率密度估计\n",
    "\n",
    "**case study**\n",
    "* discovery clusters\n",
    "    * 第一步: 预测有多少个簇 $\\Bbb P(K|D)$, 给定数据, 预估有多少个 cluster, $K^* = \\arg_K \\max \\Bbb P(K|D)$\n",
    "    * 第二步: 预测哪些点 属于哪些簇, 对于生成模型来说, 假设 当前点是由 某个簇以 某种分布生成的. \n",
    "    \n",
    "* Discovery latent factors (将高维数据 映射到 低维数据上, lower dimensional subspace which captures the “essence” of the data)\n",
    "    * The motivation behind this technique is that although the data may appear high dimensional, there may only be a small number of degrees of variability, corresponding to **latent factors**\n",
    "\n",
    "* Discovering graph structure\n",
    "\n",
    "* matrix completion\n",
    "    * image impainting\n",
    "    * Collaborative filtering\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "**Parametric vs non-parametric models**\n",
    "* 本书考虑的模型为 $p(x)$ 和 $p(y|x)$ , 分别对应着 无监督学习 和 监督学习. \n",
    "* 有多种方法定义这两种模型, 定义模型的方法最具有区别性的特征为: **模型是否具有固定数量的参数(模型的参数数量是否跟着训练样本的数量而增长)**\n",
    "* 模型具有固定数量的参数: Parametric Models\n",
    "    * advantage: 可以很快就用(训练完之后, 给定一个输入, 模型一跑, 就能得到一个输出)\n",
    "    * disadvantage: 对数据分布有很强的假设\n",
    "* 模型的参数 随着训练 样本的数量而增长: Non-Parametric Models\n",
    "    * advantage: 更加灵活\n",
    "    * disadvantage: 数据量大的时候不可行, 预测新数据可能还得把所有的数据搞在一起再学习一下\n",
    "\n",
    "\n",
    "**The curse of dimensionality**\n",
    "* dimensionality: 数据的维度\n",
    "* K nearest neighbors 对于高维数据 效果不好\n",
    "* 还需要再看一下\n",
    "\n",
    "**inductive bias**\n",
    "* 参数模型中, **经常会对数据分布进行一些假设**, 比如 我们总是会假设 $\\Bbb p(y|x), \\Bbb p(x)$ 符合某一分布, 这些假设就是模型的 inductive bias\n",
    "    * 如果 模型的 inductive bias 符合真实数据分布的话, 那么模型的效果会很好; 但是如果不符合真实数据分布的话, 模型效果就不会很好\n",
    "\n",
    "**basic function expansion**\n",
    "* $\\phi(x)=[1, x, x^2, x^3, ...]$, 特征 提取/扩展 函数\n",
    "\n",
    "\n",
    "**overfitting**\n",
    "* 将噪声数据拟合到模型中去, 导致泛化不好\n",
    "* 如何判断 overfitting\n",
    "    * logging 训练的 loss/precision 曲线, 如果训练集loss一直在降, 测试集loss先变低后有升高迹象. 说明大概率 过拟合了\n",
    "* 缓解方法\n",
    "    * https://blog.csdn.net/u012436149/article/details/69934340\n",
    "    \n",
    "**cross validation**\n",
    "* 一个数据分K份\n",
    "* 单个模型\n",
    "    * 在 K-1 份数据上训练, 然后在留下的那份数据上 测试\n",
    "    * 把 K 份数据 留一轮, 将测试的结果 平均 作为 当前模型 的performance\n",
    "* 挑个 表现最好的模型\n",
    "\n",
    "**No Free Lunch Theorem**\n",
    "* All models are wrong, but some models are useful.\n",
    "* No Free Lunch Theorem: there is no universally best model.\n",
    "    * 没有免费午餐告诉我们, 没有最好, 只有最适合. (为当前任务设计一个最适合的模型而努力)\n",
    "\n",
    "\n",
    "**case study**\n",
    "* 线性回归 (Linear Regression)\n",
    "    * 假设: 数据 呈 线性分布, 残差符合高斯分布\n",
    "    * 模型: $p(y|x, \\theta) = \\mathcal N(y|\\mu(x), \\sigma^2(x))$\n",
    "* 逻辑斯特回归 (Logistic Regression)\n",
    "    * 输出是 binary, 假设服从 条件 Bernoulli 分布\n",
    "    * $p(y| \\mathbf x, \\mathbf w)=Ber(y|\\mu(x))$\n",
    "    * $\\mu(x) = \\text{sigmoid} (\\mathbf w^T \\mathbf x)$\n",
    "* 总结: 从 线性回归 和 逻辑斯特回归 来看, 假设一般由两个部分构成. 一个是 给定输入,输出符合一个分布, 这个是概率假设 $p(y|\\mathbf x)$. 另一个是, 由于有了一个概率假设, 概率分布的参数就得由 一个模型计算出来, 这算第二个假设, 比如: 高斯/伯努利 分布的均值假设是 $u(x)=\\mathbf w^T\\mathbf x$\n",
    "\n",
    "## Exercises\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
