{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**机器学习分类** (按照数据集和要做的任务进行分类)\n",
    "* 监督学习\n",
    "* 无监督学习\n",
    "* 半监督学习\n",
    "* 强化学习\n",
    "\n",
    "**数据可视化** \n",
    "* 标签分布可视化 (使用直方图看看分布就好了)\n",
    "* 特征分布可视化 (可以根据特征所属的不同的标签 标色, 一般使用 scatter)\n",
    "\n",
    "## 监督学习\n",
    "$\\Bbb P(y| \\mathbf x_i, D)$, 后验概率\n",
    "\n",
    "## 无监督学习\n",
    "> knowledge discovery\n",
    "\n",
    "* 密度估计(density estimation)\n",
    "* vae\n",
    "* gan\n",
    "* normalizing flow (这个是不是属于 density estimation 中的? 确定一下)\n",
    "\n",
    "\n",
    "**概率估计**\n",
    "\n",
    "建模为 : $\\Bbb P(\\mathbf x_i|\\mathbf \\theta)$, 给定一个样本,能够得到其 概率\n",
    "* 注意和 监督学习的区别 :  $\\Bbb P(\\mathbf x_i|\\mathbf \\theta)$ VS $\\Bbb P(y| \\mathbf x_i, D)$\n",
    "* 监督学习: 目标 条件概率密度估计\n",
    "* 无监督学习: 目标为 无条件 概率密度估计\n",
    "\n",
    "**case study**\n",
    "* discovery clusters\n",
    "    * 第一步: 预测有多少个簇 $\\Bbb P(K|D)$, 给定数据, 预估有多少个 cluster, $K^* = \\arg_K \\max \\Bbb P(K|D)$\n",
    "    * 第二步: 预测哪些点 属于哪些簇, 对于生成模型来说, 假设 当前点是由 某个簇以 某种分布生成的. \n",
    "    \n",
    "* Discovery latent factors (将高维数据 映射到 低维数据上, lower dimensional subspace which captures the “essence” of the data)\n",
    "    * The motivation behind this technique is that although the data may appear high dimensional, there may only be a small number of degrees of variability, corresponding to **latent factors**\n",
    "\n",
    "* Discovering graph structure\n",
    "\n",
    "* matrix completion\n",
    "    * image impainting\n",
    "    * Collaborative filtering\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "**Parametric vs non-parametric models**\n",
    "* 本书考虑的模型为 $p(x)$ 和 $p(y|x)$ , 分别对应着 无监督学习 和 监督学习. \n",
    "* 有多种方法定义这两种模型, 定义模型的方法最具有区别性的特征为: **模型是否具有固定数量的参数(模型的参数数量是否跟着训练样本的数量而增长)**\n",
    "* 模型具有固定数量的参数: Parametric Models\n",
    "    * advantage: 可以很快就用(训练完之后, 给定一个输入, 模型一跑, 就能得到一个输出)\n",
    "    * disadvantage: 对数据分布有很强的假设\n",
    "* 模型的参数 随着训练 样本的数量而增长: Non-Parametric Models\n",
    "    * advantage: 更加灵活\n",
    "    * disadvantage: 数据量大的时候不可行, 预测新数据可能还得把所有的数据搞在一起再学习一下\n",
    "\n",
    "\n",
    "**The curse of dimensionality**\n",
    "* dimensionality: 数据的维度\n",
    "* K nearest neighbors 对于高维数据 效果不好"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
