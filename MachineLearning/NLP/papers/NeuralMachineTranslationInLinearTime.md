# NeuralMachineTranslationInLinearTime

> it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization



> We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.



> The larger the distance, the harder it is to learn the dependencies between the tokens

