# 过拟合问题

**什么是过拟合**

* 训练误差非常小，验证集误差非常大（现象）
* 模型能力过强，拟合了噪声（本质）

----

**什么原因导致过拟合**

* 模型复杂度比较高，可以完美的拟合训练数据



## 过拟合的解决方法

**从降低模型复杂度的方向考虑**

* 减少参数数量
* L1 正则， L2 正则
* 输入加噪声（等价于 L2 正则）
* 权重加噪声（会使得 激活值非 0 及 1, 这么来控制模型复杂度）

----

**多模型融合的角度**

* dropout
* 集成方法
* 贝叶斯神经网络（对权值采样，然后对预测的结果求平均）

----

**其它**

* 获取更多的数据
* 选择正确的模型

----

**为了 L1 会使得参数稀疏**

考虑整体 `loss_func = some_loss + l1loss` ，两个 **loss** 相加得到一个相对较小的结果。

考虑 **满足数据的 参数的解空间** 时  $\text{l1_norm}(x) = c$   的 $c$ 的最小值。**由于 $\text{l1_norm}(x) = c$ 构成的区域形状比较尖。**  所以导致了稀疏性。



[感觉这个PPT说的更有道理 https://davidrosenberg.github.io/ml2015/docs/2b.L1L2-regularization.pdf](https://davidrosenberg.github.io/ml2015/docs/2b.L1L2-regularization.pdf)



[这个随便看看就好 https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a](https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a)



# 梯度消失与梯度爆炸

**什么是梯度消失**

* 梯度为 0， 导致模型无法训练



**什么导致了梯度消失**

* activation function for the neural net gets saturated, 激活函数饱和了，梯度为 0



**如何解决梯度消失**

* 激活函数饱和： `sigmoid` 换成 `ReLU/LeakyReLU`
* ​

