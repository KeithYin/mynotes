# Sarsa Q-Learning DQN 对比总结



## Sarsa

> on-policy control

```python
Initialize Q(s,a) arbitrarily

for episode in range(M):
    Initialize s
    choose a from s using policy detived from Q (e.g. epsilon-greedy)
    
    for t in range(UntilEnd):
        Take action a_t, observe r_t, s_t+1
        quadruple (s_t, a_t, r_t, s_t+1) # this is generated by epsilon-greedy
        
        ## 三个算法的区别就在下面
        choose a_t+1 from s_t+1 using policy derived from Q (e.g. epsilon-greedy)
  	    ## 使用 epsilon-greedy 获得的数据来 estimate epsilon-greedy policy，所以是on-policy
        Q(s_t,a_t) <- Q(s_t,a_t)+ alpha*(r_t+gamma*Q(s_t+1, a_t+1)-Q(s_t,a_t))
        
        s_t <- s_t+1
        a_t <- a_t+1
```



## Q-Learning

> off-policy control

```python
Initialize Q(s,a) arbitrarily

for episode in range(M):
    Initialize s
    choose a_t from s_t using policy detived from Q (e.g. epsilon-greedy)
    for t in range(UntilEnd):
        Take action a_t, observe r_t, s_t+1
        quadruple (s_t, a_t, r_t, s_t+1) # this is generated by epsilon-greedy
        
        ## 注意在 Q—Learning 中，target-policy 是 greedy-policy
        ## bahavior-policy 是 epsilon-greedy
        choose a_t+1 from s_t+1 using greedy-policy derived from Q
        ## 使用 epsilon-greedy 获得的数据来 estimate greedy policy，所以是off-policy
        Q(s_t,a_t) <- Q(s_t,a_t)+ alpha*(r_t+gamma*Q(s_t+1, a_t+1)-Q(s_t,a_t))
        
        s_t <- s_t+1
        choose a_t+1 from s_t+1 using policy detived from Q (e.g. epsilon-greedy)
        a_t <- a_t+1
```



## DQN

> off-policy control

```python
Initialize Q(s,a) arbitrarily # 这里是初始化 Q-network 的参数，而不是之前算法的lookup-table了

for episode in range(M):
    Initialize s
    choose a from s using policy detived from Q (e.g. epsilon-greedy)
    
    for t in range(UntilEnd):
        Take action a_t, observe r_t, s_t+1
        quadruple (s_t, a_t, r_t, s_t+1) # this is generated by epsilon-greedy
        put the quadruple to replay memory
        
        ## 注意在 DQN 中，target-policy 是 greedy-policy
        ## bahavior-policy 是 experience replay
        Sample minibatch from relplay memory
        ## 使用采出来的数据来 estimate greedy policy。所以叫 off-policy
        Q(s_j,a_j) <- Q(s_j,a_j)+ alpha*(r_j+gamma*Q(s_j+1, a_j+1)-Q(s_j,a_j))
        
        s_t <- s_t+1
        choose a_t+1 from s_t+1 using policy detived from Q (e.g. epsilon-greedy)
        a_t <- a_t+1 
```

