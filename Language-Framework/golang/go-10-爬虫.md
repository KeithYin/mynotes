# 爬虫

[参考资料1](https://scene-si.org/2017/09/27/things-to-know-about-http-in-go/)

[一个挺好用的 模拟浏览器的库](https://surf.readthedocs.io/settings/)

```go
// net/http/httputil
dump := func(r *http.Request) {
	output, err := httputil.DumpRequest(r, true) // 可以用来帮助 debug 请求信息
	if err != nil {
		fmt.Println("Error dumping request:", err)
		return
	}
	fmt.Println(string(output))
}
```





**http工作方式， HTTP 协议在 应用层，基于 TCP**

* HTTP 是用来解析 `TCP` 层传输的数据的
* 直接用 `TCP` 进行 B/S 编程的话，需要自己写代码解析报文信息，使用 `HTTP` 协议就更简单了

```
当浏览器输入 www.baidu.com时底层发生了什么

1. client --> DNS server : 获取 www.baidu.com 对应的  url 地址
2. client 通过 url 访问 web 服务器
```



**客户端请求包**

```
#GET / HTTP/1.1   --> 请求行
Host: 127.0.0.1:8000   ---> 请求头
Connection: keep-alive ---> 请求头

```

**响应包**



**HTTP编程---服务器**

```go
package main

import (
	"fmt"
    "net/http"
)

func HandleConn(w http.ResponseWriter, req *http.Request){
    // w: 用于给客户端回复数据
    // req: 用来读取客户端发来的数据
    w.Write([]byte("hello go"))
}

func main(){
    // 注册处理函数，用户连接，自动调用指定的处理函数
    // 第一个参数代表为 哪个请求 定义的 回调函数
    http.HandleFunc("/", HandConn)
    
    // 监听绑定
    http.ListenAndServe(":8000", nil)
}
```


**客户端编程**

```go
package main
import (
	"fmt"
    "net/http"
)
func main(){
    resp, err1 := http.Get("http://www.baidu.com")
    defer resp.Body.Close()
    
    buf := make([]byte, 4*1024)
    
    for {
        n, err2 := resp.Body.Read(buf)
        if n == 0{
            break
        }
    }
}

```

### 几个抽象

* Client: 理解为 浏览器客户端
* Request: http 的请求
* Respnose: http 的响应内容


### 爬虫

* 明确目标
* 爬（将目标网站的内容爬取下来）
* 取（去掉没有用处的数据）
* 处理数据（按照我们想要的方式存储和使用）

```go
// 搞 百度贴吧
package main

import (
	"fmt"
    "net/http"
)

func getPage(url string)(result string, err error){
    resp, err1 := http.Get(url)
    if err1 != nil{
        err = err1
       	return
    }
    
    defer resp.Body.Close()
    
    // 读取网页 body 内容
    buf := make([]byte, 4*1024)
    
    for{
        n, _ := resp.Body.Read(buf)
    }
    
}

func main(){
    for i:=0; i<5; i++{
        getPage("http://www.baidu.com")
    }
}
```



# Details

`http`包对于 HTTP 请求的三个抽象：`http.Client`, `http.Request`, `http.RoundTripper`



* `http.Client`
  * http client：目的是发 http 请求包，接收 http 响应包
  * The Client's Transport typically has internal state (cached TCP connections), so Clients should be reused instead of created as needed. 
* `cookieJar` 用来管理 `cookie` : 因为在访问网站的时候, 服务器有时候会 `Set-Cookies` , 这些值以及`cookies` 所对应的`host-url` 就会被 `cookieJar` 管理起来, 再访问该 网站的时候, 就会像浏览器一样 带过去. 
  
* `http.Request` 

  * 作为 client request：可以设置 http 请求包的一些 值
  * 作为 server request：可以访问 http 请求包的 一些信息

* `http.RoundTripper`, 是个接口:

  * execute a single HTTP transaction, obtaining the Response for a given Request.
  * `RoundTrip(*Request) (*Response, error)`

* `http.Transport`: `RoundTripper`的一个实现
  * 配置 HTTP 请求的一些偏底层的属性


```go
var DefaultTransport RoundTripper = &Transport{
    Proxy: ProxyFromEnvironment,
    DialContext: (&net.Dialer{
        Timeout:   30 * time.Second,
        KeepAlive: 30 * time.Second,
        DualStack: true,
    }).DialContext,
    MaxIdleConns:          100,
    IdleConnTimeout:       90 * time.Second,
    TLSHandshakeTimeout:   10 * time.Second,
    ExpectContinueTimeout: 1 * time.Second,
}
```



**Demo3：设置 cookie**

[cookieJar用法](https://gist.github.com/HugoPresents/a8a44c3c4cd514052952)

* `http.Request.Header` 是个 `map[string][]string` 类型，
* `Header` 里面是用来设置 `HTTP` 请求头中的参数的

```go
// Header 是个 map[string][]string
request, _ := http.Newrequest("GET", aidishengUrl.String(), nil)

// 第一种设置cookie 的方法
request.Header.Set("Cookie", "...; ...; ...;")

// 第二种, 如果有多个值, 如果有多个key-val, 可以循环添加
baiduIdCookie := &http.Cookie{Name: "BAIDUID", Value: "C03E261"}
request.AddCookie(baiduIdCookie)
```

**Demo4: 使用cookieJar**

```go
func LoginAndOpen(urlStr string) *browser.Browser {
	bow := surf.NewBrowser()
	bow.SetUserAgent(agent.Chrome())
  // 在bow生命周期期间, 这个cj会管理 bow 经历的所有 cookie
  // Response中有Set-cookie 的时候, 将新的 cookie添加或者更新到 cookieJar
  // 请求的时候, 如果在cookieJar中能找到 对应host 的cookie, 就带到request中
	cj, _ := cookiejar.New(nil)

	bow.SetCookieJar(cj)

	// 会redirect 到登录界面, 直接执行登录逻辑
	err := bow.Open(urlStr)

	fm, err2 := bow.Form("form#form-email")
	fm.Input("username", "")
	fm.Input("password", "")

	if fm.Submit() != nil {
		panic(err)
	}
	// 登录成功后会自动设置 cookie, 然后再打开一遍就行了
	bow.Open(urlStr)
	return bow
}
```



**Demo4: 使用PostForm**

```go
// 使用这个重点是, 1. 确定post的action url; 2. 确定post请求时传入的数据. 
// 确定post的传入数据, 这个可以从 chrome 开发者工具中拿到
func usingPostForm(){
	body := url.Values{
		"cmd":        []string{fmt.Sprintf("%d", cmd)},
		"exp_id":     []string{expId},
		"user":       []string{"goodday"},
		"cut_reason": []string{"anyreason_you_want"}}

	bow.PostForm("url", body)
}
```





需要注意的几个点

* **Clients and Transports are safe for concurrent use by multiple goroutines and for efficiency should only be created once and re-used. If the Body is not closed, the Client's underlying RoundTripper (typically Transport) may not be able to re-use a persistent TCP connection to the server for a subsequent "keep-alive" request.**
* **The client must close the response body when finished with it**
  * `defer response.Body.Close()`




对于 `http` 来说, **一切皆为请求**, 想要啥操作一定找到对应的请求 `url`. 对着 `url` 发就行了

* 对于请求需要注意的一点就是: 确定时 `GET` 还是 `POST`, 然后确定传过去的值是啥 (`chrome` 开发者工具是个好帮手)
* 即使网页是 `js` 来动态生成的. 那么生成网页的 数据 也是通过请过获得的, 然后由 `js` 操作 `DOM` 实现的.

## 碰到的问题

```go
fenfaData, err := http.Get(uri)
if err != nil {
	fmt.Println(err)
}
defer fenfaData.Body.Close()

var info string
for {
	n, err := fenfaData.Body.Read(buf)
	if err != nil { //使用此判断, 会直接EOF, 不知道为啥
		fmt.Println("error: ", err)
		break
	}
	if n == 0 { // 只使用 n == 0 判断是可以正常的获取数据的
		fmt.Println("n=0, EOF")
		break
	}
	curLine := string(buf)
	info += curLine
}
```


